@article{luke2021analysis,
  title={Analysis methods for measuring passive auditory fNIRS responses generated by a block-design paradigm},
  author={Luke, Robert and Larson, Eric and Shader, Maureen J and Innes-Brown, Hamish and Van Yper, Lindsey and Lee, Adrian KC and Sowman, Paul F and McAlpine, David},
  journal={Neurophotonics},
  volume={8},
  number={2},
  pages={025008},
  year={2021},
  publisher={International Society for Optics and Photonics}
}

@article{Schönwiesner2021,
  doi = {10.21105/joss.03284},
  url = {https://doi.org/10.21105/joss.03284},
  year = {2021},
  publisher = {The Open Journal},
  volume = {6},
  number = {62},
  pages = {3284},
  author = {Marc Schönwiesner and Ole Bialas},
  title = {s(ound)lab: An easy to learn Python package for designing and running psychoacoustic experiments.},
  journal = {Journal of Open Source Software}
}


@misc{pychoacoustics,
  author = {Samuele Carcagno},
  title = {Pychoacoustics},
  year = {2012},
  publisher = {​GitHub},
  journal = {​GitHub repository},
  howpublished = {​https://github.com/sam81/pychoacoustics}
}

@article{psychopy2,
	title = {PsychoPy2: Experiments in behavior made easy},
	volume = {51},
	issn = {1554-3528},
	doi = {10.3758/s13428-018-01193-y},
	number = {1},
	journal = {Behavior Research Methods},
	author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and Höchenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindeløv, Jonas Kristoffer},
	year = {2019},
	pages = {195--203}
}

@inproceedings{mcfee2015librosa,
  title={librosa: Audio and music signal analysis in python},
  author={McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel PW and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
  booktitle={Proceedings of the 14th python in science conference},
  volume={8},
  year={2015},
  doi = {10.25080/majora-7b98e3ed-003}
}

@misc{python-sofa,
  author = {Jannika Lossner},
  title = {Spatially Oriented Format for Acoustics (SOFA) API for Python},
  year = {2019},
  publisher = {​GitHub},
  journal = {​GitHub repository},
  howpublished = {​https://github.com/spatialaudio/python-sofa}
}

@misc{sampled-signals,
  author = { Spencer Russell },
  title = {SampledSignals.jl},
  year = {2016},
  publisher = {​GitHub},
  journal = {​GitHub repository},
  howpublished = {​https://github.com/JuliaAudio/SampledSignals.jl}
}

@article{gramfort2014mne,
  title={MNE software for processing MEG and EEG data},
  author={Gramfort, Alexandre and Luessi, Martin and Larson, Eric and Engemann, Denis A and Strohmeier, Daniel and Brodbeck, Christian and Parkkonen, Lauri and H{\"a}m{\"a}l{\"a}inen, Matti S},
  journal={Neuroimage},
  volume={86},
  pages={446--460},
  year={2014},
  publisher={Elsevier}
}

@article{oostenveld2011fieldtrip,
  title={FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data},
  author={Oostenveld, Robert and Fries, Pascal and Maris, Eric and Schoffelen, Jan-Mathijs},
  journal={Computational intelligence and neuroscience},
  volume={2011},
  year={2011},
  publisher={Hindawi}
}

@article{bezanson2017julia,
  title={Julia: A fresh approach to numerical computing},
  author={Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  journal={SIAM review},
  volume={59},
  number={1},
  pages={65--98},
  year={2017},
  publisher={SIAM}
}

@misc{sapozhnykov2019wind,
  title={Wind noise measurement},
  author={Sapozhnykov, Vitaliy and Harvey, Thomas Ivan and Robert, LUKE},
  year={2019},
  month=dec # "~10",
  publisher={Google Patents},
  note={US Patent 10,504,537}
}

@misc{robert2019blocked,
  title={Blocked microphone detection},
  author={Luke, Robert and Sapozhnykov, Vitaliy and Harvey, Thomas Ivan},
  year={2019},
  month=sep # "~10",
  publisher={Google Patents},
  note={US Patent 10,412,518}
}

@misc{sapozhnykov2020headset,
  title={Headset on ear state detection},
  author={Sapozhnykov, Vitaliy and Harvey, Thomas Ivan and Erfaniansaeedi, Nafiseh and Robert, LUKE},
  year={2020},
  month=oct # "~20",
  publisher={Google Patents},
  note={US Patent 10,812,889}
}

@article{luke2016kalman,
  title={Kalman filter based estimation of auditory steady state response parameters},
  author={Luke, Robert and Wouters, Jan},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  volume={25},
  number={3},
  pages={196--204},
  year={2016},
  publisher={IEEE}
}

@article{CEOLINI2020117282,
title = {Brain-informed speech separation (BISS) for enhancement of target speaker in multitalker speech perception},
journal = {NeuroImage},
volume = {223},
pages = {117282},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117282},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920307680},
author = {Enea Ceolini and Jens Hjortkjær and Daniel D.E. Wong and James O’Sullivan and Vinay S. Raghavan and Jose Herrero and Ashesh D. Mehta and Shih-Chii Liu and Nima Mesgarani},
keywords = {EEG, Neuro-steered, Cognitive control, Speech separation, Deep learning, Hearing aid},
abstract = {Hearing-impaired people often struggle to follow the speech stream of an individual talker in noisy environments. Recent studies show that the brain tracks attended speech and that the attended talker can be decoded from neural data on a single-trial level. This raises the possibility of “neuro-steered” hearing devices in which the brain-decoded intention of a hearing-impaired listener is used to enhance the voice of the attended speaker from a speech separation front-end. So far, methods that use this paradigm have focused on optimizing the brain decoding and the acoustic speech separation independently. In this work, we propose a novel framework called brain-informed speech separation (BISS)11BISS: brain-informed speech separation. in which the information about the attended speech, as decoded from the subject’s brain, is directly used to perform speech separation in the front-end. We present a deep learning model that uses neural data to extract the clean audio signal that a listener is attending to from a multi-talker speech mixture. We show that the framework can be applied successfully to the decoded output from either invasive intracranial electroencephalography (iEEG) or non-invasive electroencephalography (EEG) recordings from hearing-impaired subjects. It also results in improved speech separation, even in scenes with background noise. The generalization capability of the system renders it a perfect candidate for neuro-steered hearing-assistive devices.}
}
